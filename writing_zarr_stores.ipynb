{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "516d0fd0",
   "metadata": {},
   "source": [
    "### **Using SNAP with ArgoWorkflows**\n",
    "\n",
    "This notebook will show how to write INCA data to an exising zarr store. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0871cbe",
   "metadata": {},
   "source": [
    "**First some imports and global settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98851abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from hera.workflows import models, CronWorkflow, script, Artifact, Parameter, DAG, Steps, Step, NoneArchiveStrategy, Workflow\n",
    "from hera.shared import global_config\n",
    "\n",
    "global_config.host = \"https://dev.services.eodc.eu/workflows/\"\n",
    "global_config.namespace = \"<YOUR NAMESPACE>\"\n",
    "global_config.token = \"<YOUR TOKEN>\"\n",
    "global_config.image = \"ghcr.io/eodcgmbh/cluster_image:2025.2.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716ba6e6",
   "metadata": {},
   "source": [
    "**Setting up Volume**\n",
    "\n",
    "We need to set up our the connection to the volume we want to write to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8596c4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfs_volume = models.Volume(\n",
    "    name=\"eodc-mount\",\n",
    "    persistent_volume_claim={\"claimName\": \"eodc-nfs-claim\"},\n",
    "    )\n",
    "\n",
    "security_context = {\"runAsUser\": <YOUR UID>,\n",
    "                    \"runAsGroup\": <YOUR GID>}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcab5c1",
   "metadata": {},
   "source": [
    "**Writing scripts**\n",
    "\n",
    "For this example we will write INCA data downloaded from the Geosphere Datahub to an existing Zarr store on the EODC NFS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16049a5e",
   "metadata": {},
   "source": [
    "First, we need to extent our zarr store, so we can write new data to it. INCA data is available hourly, so we will extend it to the time when running the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9350f31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@script(volume_mounts=[models.VolumeMount(name=\"eodc-mount\", mount_path=\"/eodc\")])\n",
    "\n",
    "def extend_time_dimension(store_path: str = \"/eodc/products/eodc/geosphere_inca/INCA.zarr\"):\n",
    "    import datetime\n",
    "    import numpy as np\n",
    "    import zarr\n",
    "\n",
    "    # In our zarr stores time coordinate, the origin is 2011-03-15, the time steps are set as hours between that\n",
    "    # origin time and now. \n",
    "    now = datetime.datetime.now()\n",
    "    now_np = np.datetime64(now).astype('datetime64[h]')\n",
    "    origin = np.datetime64(\"2011-03-15T00:00:00\").astype(\"datetime64[h]\")\n",
    "\n",
    "    # The new extent and shape for the time coordinate is set\n",
    "    new_shape = int((now_np-origin).astype(int))\n",
    "    new_extent = np.arange(0,new_shape,1)\n",
    "\n",
    "    # Connect to the zarr store\n",
    "    store = zarr.storage.LocalStore(store_path)\n",
    "    group = zarr.group(store=store)\n",
    "\n",
    "    # As not only the time coordinate, but also the data arrays have to be extended, they are filtered out in\n",
    "    # this step\n",
    "    array_names=set(group.array_keys())\n",
    "    coords = {\"time\", \"x\", \"y\"}\n",
    "    data_arrays = array_names-coords\n",
    "\n",
    "    # The time coordinate as well as the data arrays are resized to the new shape.\n",
    "    group[\"time\"].resize(new_shape)\n",
    "    for array in data_arrays:\n",
    "        group_shape  = group[array].shape\n",
    "        group[array].resize((new_shape, group_shape[1], group_shape[2]))\n",
    "\n",
    "    # The metadata has to be consolidated to make sure it can be properly read.\n",
    "    zarr.consolidate_metadata(store)\n",
    "\n",
    "    # Fresh connect to the zarr store in order to write the updated time coordinates to it.\n",
    "    store = zarr.storage.LocalStore(store_path)\n",
    "    group = zarr.group(store=store)\n",
    "\n",
    "    group[\"time\"][:]=new_extent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbff2dc",
   "metadata": {},
   "source": [
    "Next we download data from the datahub and save it as an Artifact to pass it to the next step. There is seperate file for every month but the data is updated hourly, the files have the Year/Month in the name. As we will run the workflow every day, we will choose to get the the file with the Year/Month of the previous day to make sure that we choose the file from the previous month on the first of the next month, and fill the previous month with data completely. We also have eight parameters for the INCA data which are also part of the filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2768af",
   "metadata": {},
   "outputs": [],
   "source": [
    "@script(outputs=Artifact(name=\"inca-file\", path=\"/tmp/INCA.nc\", archive=NoneArchiveStrategy()))\n",
    "\n",
    "def inca_download(param: str):\n",
    "    from urllib.request import urlretrieve\n",
    "    import datetime\n",
    "\n",
    "    ym = (datetime.date.today()-datetime.timedelta(days=1)).strftime(\"%Y%m\")\n",
    "\n",
    "    url = f\"https://public.hub.geosphere.at/datahub/resources/inca-v1-1h-1km/filelisting/{param}/INCAL_HOURLY_{param}_{ym}.nc\"\n",
    "    urlretrieve(url, f\"/tmp/INCA.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f033d4",
   "metadata": {},
   "source": [
    "Finally, we want to write the data from the netCDF file to the zarr store. As we want to update daily and only have files with monthly data we will overwrite existing data in the store, this is only feasible because the files are relatively small and processing is fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ea397c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@script(inputs=Artifact(name=\"inca-file\", path=\"/tmp/INCA.nc\"),\n",
    "        volume_mounts=[models.VolumeMount(name=\"eodc-mount\", mount_path=\"/eodc\")])\n",
    "\n",
    "def inca_write(param: str, store_path: str=\"/eodc/private/tempearth/INCA.zarr\"):\n",
    "    import xarray as xr\n",
    "    import numpy as np\n",
    "    import zarr\n",
    "    import pandas as pd\n",
    "\n",
    "    artifact_path = f\"/tmp/INCA.nc\"\n",
    "\n",
    "    # To make it easier when writing the data to the store we define a simple function to get the correct indices\n",
    "    # of where to write to\n",
    "    def get_idx(array1, array2):\n",
    "        min_idx = np.where(array1 == array2[0])[0][0]\n",
    "        max_idx = np.where(array1 == array2[-1])[0][0] + 1\n",
    "        return min_idx, max_idx\n",
    "\n",
    "    # Connect to the store\n",
    "    store = zarr.storage.LocalStore(store_path)\n",
    "    group = zarr.group(store=store)\n",
    "\n",
    "    # Get relevant data from the store\n",
    "    dtype = group[param].dtype\n",
    "    fill_value = group[param].attrs.get('_FillValue')\n",
    "    freq = group.attrs.get('freq')\n",
    "\n",
    "    x_extent = group[\"x\"][:]\n",
    "    y_extent = group[\"y\"][:]\n",
    "    origin = xr.open_zarr(store_path).time[0].values\n",
    "\n",
    "    # Load the geosphere data\n",
    "    data = xr.open_dataset(artifact_path, mask_and_scale=False).load()\n",
    "\n",
    "    # Get the indices of where to write to \n",
    "    x_min, x_max = get_idx(x_extent, data[\"x\"].values)\n",
    "    y_min, y_max = get_idx(y_extent, data[\"y\"].values)\n",
    "\n",
    "    time_min, time_max = data.time.values[0].astype(\"datetime64[h]\"), data.time.values[-1].astype(\"datetime64[h]\") + 1\n",
    "    time_delta_min, time_delta_max = (time_min - origin).astype(\"int64\"), (time_max - origin).astype(\"int64\")\n",
    "\n",
    "    # In case the data from the netCDF file is incomplete we need to fill missing time steps with noData values\n",
    "    # in order to avoid potential errors when writing to the zarr store.\n",
    "    full_range = pd.date_range(time_min, time_max, freq=freq).values.astype(\"datetime64[ns]\")\n",
    "\n",
    "    for value in data.time.values:\n",
    "        if value in set(full_range):\n",
    "            continue\n",
    "        else:\n",
    "            empty_array = np.full((full_range.shape[0], data[\"x\"].values.shape[0], data[\"y\"].values.shape[0]),\n",
    "                                fill_value=fill_value, dtype=dtype)\n",
    "\n",
    "            template = xr.Dataset({f\"{param}\": ((\"time\", \"x\", \"y\"), empty_array)},\n",
    "                                  coords={\n",
    "                                    \"time\": full_range,\n",
    "                                    \"x\": data[\"x\"].values,\n",
    "                                    \"y\": data[\"y\"].values\n",
    "                                  }\n",
    "                                  )\n",
    "\n",
    "            data = data.combine_first(template)\n",
    "            break\n",
    "\n",
    "    # Finally we can write to the store.\n",
    "    group[param][time_delta_min:time_delta_max, y_min:y_max, x_min:x_max] = data[param].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2282d41c",
   "metadata": {},
   "source": [
    "**Creating the Workflow**\n",
    "\n",
    "As already mentioned we have eight parameters for which we want the writing to the store be done in parallel. Also we want the workflow to run daily.\n",
    "\n",
    "Steps act the same way as DAGs, with the difference of running in the flow the are written in. The DAG specified here acts as a template which is then used in the Steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2764af72",
   "metadata": {},
   "outputs": [],
   "source": [
    "inca_parameters = [\"RR\", \"T2M\", \"TD2M\", \"P0\", \"UU\", \"VV\", \"RH2M\", \"GL\"]\n",
    "\n",
    "# At first a CronWorkflow is created with the necessary inputs.\n",
    "with CronWorkflow(\n",
    "    generate_name=\"inca-zarr-\",\n",
    "    schedule=\"0 6 * * *\",\n",
    "    volumes = [nfs_volume],\n",
    "    security_context=security_context,\n",
    "    entrypoint=\"workflow\"\n",
    ") as w:\n",
    "    \n",
    "    # Secondly, a DAG is defined which shall be executed for each parameter. The inputs are defined in the Steps below. So this DAG acts like a function being defined and executed in a different step.\n",
    "    with DAG(name=\"pipeline\", inputs=[Parameter(name=\"args\")]) as pipeline:\n",
    "        \n",
    "        # The arguments for the scripts are passed as a dictionary, wherease the arguments for the 'param' parameter are taken from the input of the DAG\n",
    "        download = inca_download(arguments={\"param\":\"{{inputs.parameters.args}}\"},)\n",
    "\n",
    "        # The Artifact written with download also has to be given to the function.\n",
    "        process = inca_write(arguments=[{\"param\": \"{{inputs.parameters.args}}\"}, \n",
    "                                        download.get_artifact(\"inca-file\").with_name(\"inca-file\")],)\n",
    "\n",
    "        # Here the sequence of the steps is defined\n",
    "        download >> process\n",
    "\n",
    "    # As we defined \"workflow\" as the entrypoint in the CronWorkflow this part gets executed first, in contrast to DAGs Steps will be executed in the order they are in\n",
    "    with Steps(name=\"workflow\"):\n",
    "        # First the time dimension is extended in the zarr store\n",
    "        extend_time_dimension()\n",
    "\n",
    "        # Now the DAG is executed, it is used as a template, passing the inca_parameters as with_param and using \"{{item}}\" in arguments the DAG will be executed parallel for each parameter.\n",
    "        Step(name=\"parallel-pipelines\", template=pipeline, with_param=inca_parameters, arguments={\"args\":\"{{item}}\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61f9933",
   "metadata": {},
   "source": [
    "**Submitting the Workflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb7181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.create()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zarr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
